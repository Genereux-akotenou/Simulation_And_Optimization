{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download Stock Data\n",
    "def download_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data['Close'].values, data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess Data\n",
    "def preprocess_data(data, sequence_length):\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(data[i+sequence_length])\n",
    "\n",
    "    return np.array(sequences), np.array(targets), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Dataset Class\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Build the Transformer Model\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.fc_out(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b. Build the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train the Model\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(-1))  # Add extra dimension for feature\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer Model for GOOG\n",
      "Epoch 1/20, Loss: 0.0452\n",
      "Epoch 2/20, Loss: 0.0184\n",
      "Epoch 3/20, Loss: 0.0191\n",
      "Epoch 4/20, Loss: 0.0064\n",
      "Epoch 5/20, Loss: 0.0100\n",
      "Epoch 6/20, Loss: 0.0052\n",
      "Epoch 7/20, Loss: 0.0041\n",
      "Epoch 8/20, Loss: 0.0100\n",
      "Epoch 9/20, Loss: 0.0052\n",
      "Epoch 10/20, Loss: 0.0036\n",
      "Epoch 11/20, Loss: 0.0075\n",
      "Epoch 12/20, Loss: 0.0036\n",
      "Epoch 13/20, Loss: 0.0039\n",
      "Epoch 14/20, Loss: 0.0044\n",
      "Epoch 15/20, Loss: 0.0019\n",
      "Epoch 16/20, Loss: 0.0030\n",
      "Epoch 17/20, Loss: 0.0058\n",
      "Epoch 18/20, Loss: 0.0036\n",
      "Epoch 19/20, Loss: 0.0044\n",
      "Epoch 20/20, Loss: 0.0028\n",
      "Training LSTM Model for GOOG\n",
      "Epoch 1/20, Loss: 0.0468\n",
      "Epoch 2/20, Loss: 0.0560\n",
      "Epoch 3/20, Loss: 0.0428\n",
      "Epoch 4/20, Loss: 0.0154\n",
      "Epoch 5/20, Loss: 0.0162\n",
      "Epoch 6/20, Loss: 0.0086\n",
      "Epoch 7/20, Loss: 0.0092\n",
      "Epoch 8/20, Loss: 0.0062\n",
      "Epoch 9/20, Loss: 0.0069\n",
      "Epoch 10/20, Loss: 0.0046\n",
      "Epoch 11/20, Loss: 0.0066\n",
      "Epoch 12/20, Loss: 0.0059\n",
      "Epoch 13/20, Loss: 0.0045\n",
      "Epoch 14/20, Loss: 0.0075\n",
      "Epoch 15/20, Loss: 0.0037\n",
      "Epoch 16/20, Loss: 0.0043\n",
      "Epoch 17/20, Loss: 0.0060\n",
      "Epoch 18/20, Loss: 0.0041\n",
      "Epoch 19/20, Loss: 0.0032\n",
      "Epoch 20/20, Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer Model for BTC\n",
      "Epoch 1/20, Loss: 0.0180\n",
      "Epoch 2/20, Loss: 0.0103\n",
      "Epoch 3/20, Loss: 0.0092\n",
      "Epoch 4/20, Loss: 0.0073\n",
      "Epoch 5/20, Loss: 0.0059\n",
      "Epoch 6/20, Loss: 0.0064\n",
      "Epoch 7/20, Loss: 0.0043\n",
      "Epoch 8/20, Loss: 0.0046\n",
      "Epoch 9/20, Loss: 0.0038\n",
      "Epoch 10/20, Loss: 0.0017\n",
      "Epoch 11/20, Loss: 0.0054\n",
      "Epoch 12/20, Loss: 0.0027\n",
      "Epoch 13/20, Loss: 0.0024\n",
      "Epoch 14/20, Loss: 0.0020\n",
      "Epoch 15/20, Loss: 0.0030\n",
      "Epoch 16/20, Loss: 0.0019\n",
      "Epoch 17/20, Loss: 0.0023\n",
      "Epoch 18/20, Loss: 0.0011\n",
      "Epoch 19/20, Loss: 0.0028\n",
      "Epoch 20/20, Loss: 0.0024\n",
      "Training LSTM Model for BTC\n",
      "Epoch 1/20, Loss: 0.0201\n",
      "Epoch 2/20, Loss: 0.0231\n",
      "Epoch 3/20, Loss: 0.0184\n",
      "Epoch 4/20, Loss: 0.0190\n",
      "Epoch 5/20, Loss: 0.0035\n",
      "Epoch 6/20, Loss: 0.0039\n",
      "Epoch 7/20, Loss: 0.0050\n",
      "Epoch 8/20, Loss: 0.0056\n",
      "Epoch 9/20, Loss: 0.0028\n",
      "Epoch 10/20, Loss: 0.0057\n",
      "Epoch 11/20, Loss: 0.0040\n",
      "Epoch 12/20, Loss: 0.0039\n",
      "Epoch 13/20, Loss: 0.0030\n",
      "Epoch 14/20, Loss: 0.0026\n",
      "Epoch 15/20, Loss: 0.0032\n",
      "Epoch 16/20, Loss: 0.0022\n",
      "Epoch 17/20, Loss: 0.0013\n",
      "Epoch 18/20, Loss: 0.0013\n",
      "Epoch 19/20, Loss: 0.0015\n",
      "Epoch 20/20, Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer Model for NVDA\n",
      "Epoch 1/20, Loss: 0.0554\n",
      "Epoch 2/20, Loss: 0.0124\n",
      "Epoch 3/20, Loss: 0.0036\n",
      "Epoch 4/20, Loss: 0.0072\n",
      "Epoch 5/20, Loss: 0.0043\n",
      "Epoch 6/20, Loss: 0.0026\n",
      "Epoch 7/20, Loss: 0.0049\n",
      "Epoch 8/20, Loss: 0.0049\n",
      "Epoch 9/20, Loss: 0.0042\n",
      "Epoch 10/20, Loss: 0.0031\n",
      "Epoch 11/20, Loss: 0.0059\n",
      "Epoch 12/20, Loss: 0.0041\n",
      "Epoch 13/20, Loss: 0.0025\n",
      "Epoch 14/20, Loss: 0.0019\n",
      "Epoch 15/20, Loss: 0.0017\n",
      "Epoch 16/20, Loss: 0.0031\n",
      "Epoch 17/20, Loss: 0.0020\n",
      "Epoch 18/20, Loss: 0.0012\n",
      "Epoch 19/20, Loss: 0.0016\n",
      "Epoch 20/20, Loss: 0.0014\n",
      "Training LSTM Model for NVDA\n",
      "Epoch 1/20, Loss: 0.1019\n",
      "Epoch 2/20, Loss: 0.0460\n",
      "Epoch 3/20, Loss: 0.0067\n",
      "Epoch 4/20, Loss: 0.0058\n",
      "Epoch 5/20, Loss: 0.0063\n",
      "Epoch 6/20, Loss: 0.0064\n",
      "Epoch 7/20, Loss: 0.0033\n",
      "Epoch 8/20, Loss: 0.0041\n",
      "Epoch 9/20, Loss: 0.0017\n",
      "Epoch 10/20, Loss: 0.0050\n",
      "Epoch 11/20, Loss: 0.0047\n",
      "Epoch 12/20, Loss: 0.0023\n",
      "Epoch 13/20, Loss: 0.0019\n",
      "Epoch 14/20, Loss: 0.0027\n",
      "Epoch 15/20, Loss: 0.0028\n",
      "Epoch 16/20, Loss: 0.0023\n",
      "Epoch 17/20, Loss: 0.0021\n",
      "Epoch 18/20, Loss: 0.0023\n",
      "Epoch 19/20, Loss: 0.0021\n",
      "Epoch 20/20, Loss: 0.0011\n",
      "Overall Evaluation:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 6. Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    START_DATE = '2022-01-01'\n",
    "    END_DATE = '2024-01-01'\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    INPUT_SIZE = 1\n",
    "    D_MODEL = 64\n",
    "    NHEAD = 4\n",
    "    NUM_LAYERS = 2\n",
    "    DIM_FEEDFORWARD = 128\n",
    "    DROPOUT = 0.1\n",
    "    HIDDEN_SIZE = 64\n",
    "    OUTPUT_SIZE = 1\n",
    "    tickers = ['GOOG', 'BTC', 'NVDA']\n",
    "    all_rmse = []\n",
    "    all_acc  = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        data, dates = download_stock_data(ticker, START_DATE, END_DATE)\n",
    "        sequences, targets, scaler = preprocess_data(data, SEQUENCE_LENGTH)\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = StockDataset(sequences, targets)\n",
    "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # Initialize models, criterion, and optimizer\n",
    "        transformer_model = CustomTransformerModel(INPUT_SIZE, D_MODEL, NHEAD, NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
    "        lstm_model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE, DROPOUT)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        transformer_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "        lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train the models\n",
    "        print(f\"\\nTraining Transformer Model for {ticker}\")\n",
    "        train_model(transformer_model, train_loader, criterion, transformer_optimizer, EPOCHS)\n",
    "        print(f\"Training LSTM Model for {ticker}\")\n",
    "        train_model(lstm_model, train_loader, criterion, lstm_optimizer, EPOCHS)\n",
    "\n",
    "        # Evaluate the models\n",
    "        transformer_predictions = []\n",
    "        lstm_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(sequences)):\n",
    "                input_seq = torch.tensor(sequences[i], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "                transformer_pred = transformer_model(input_seq).item()\n",
    "                lstm_pred = lstm_model(input_seq).item()\n",
    "                transformer_predictions.append(transformer_pred)\n",
    "                lstm_predictions.append(lstm_pred)\n",
    "\n",
    "        # Inverse transform the predictions and targets\n",
    "        transformer_predictions = scaler.inverse_transform(np.array(transformer_predictions).reshape(-1, 1)).reshape(-1)\n",
    "        lstm_predictions = scaler.inverse_transform(np.array(lstm_predictions).reshape(-1, 1)).reshape(-1)\n",
    "        targets = scaler.inverse_transform(targets.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        transformer_rmse = np.sqrt(mean_squared_error(targets, transformer_predictions))\n",
    "        lstm_rmse = np.sqrt(mean_squared_error(targets, lstm_predictions))\n",
    "\n",
    "        # Append RMSE to list for overall evaluation\n",
    "        all_rmse.append((ticker, transformer_rmse, lstm_rmse))\n",
    "\n",
    "    # Overall evaluation\n",
    "    print(\"Overall Evaluation:\")\n",
    "    evaluation_data = []\n",
    "    range_targets = max(targets) - min(targets)\n",
    "    for ticker, transformer_rmse, lstm_rmse in all_rmse:\n",
    "        transformer_acc = (1 - (transformer_rmse / range_targets)) * 100\n",
    "        lstm_acc = (1 - (lstm_rmse / range_targets)) * 100\n",
    "        evaluation_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Transformer Model RMSE': transformer_rmse,\n",
    "            'LSTM Model RMSE': lstm_rmse,\n",
    "            'Transformer Model ACC': transformer_acc,\n",
    "            'LSTM Model ACC': lstm_acc\n",
    "        })\n",
    "    evaluation_df = pd.DataFrame(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Transformer Model RMSE</th>\n",
       "      <th>LSTM Model RMSE</th>\n",
       "      <th>Transformer Model ACC</th>\n",
       "      <th>LSTM Model ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>3.793199</td>\n",
       "      <td>4.279115</td>\n",
       "      <td>99.031903</td>\n",
       "      <td>98.907887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTC</td>\n",
       "      <td>0.503902</td>\n",
       "      <td>0.490890</td>\n",
       "      <td>99.871395</td>\n",
       "      <td>99.874715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>17.868783</td>\n",
       "      <td>18.390999</td>\n",
       "      <td>95.439543</td>\n",
       "      <td>95.306263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker  Transformer Model RMSE  LSTM Model RMSE  Transformer Model ACC  \\\n",
       "0   GOOG                3.793199         4.279115              99.031903   \n",
       "1    BTC                0.503902         0.490890              99.871395   \n",
       "2   NVDA               17.868783        18.390999              95.439543   \n",
       "\n",
       "   LSTM Model ACC  \n",
       "0       98.907887  \n",
       "1       99.874715  \n",
       "2       95.306263  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
